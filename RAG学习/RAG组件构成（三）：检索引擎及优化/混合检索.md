# 混合检索
混合检索（Hybrid Search）是一种结合了 **稀疏向量（Sparse Vectors）** 和 **密集向量（Dense Vectors）** 优势的先进搜索技术。旨在同时利用稀疏向量的关键词精确匹配能力和密集向量的语义理解能力，以克服单一向量检索的局限性，从而在各种搜索场景下提供更准确、更鲁棒的检索结果。

在本节中，我们将首先分析这两种核心向量的特性，然后探讨它们如何融合，最后通过milvus实现混合检索

## 一、稀疏向量 vs 密集向量
### 1.1 稀疏向量
稀疏向量，也常被称为“词法向量”，是基于词频统计的传统信息检索方法的数学表示。它通常是一个维度极高（与词汇表大小相当）但绝大多数元素为零的向量。它采用精准的“词袋”匹配模型，将文档视为一堆词的集合，不考虑其顺序和语法，其中向量的每一个维度都直接对应一个具体的词，非零值则代表该词在文档中的重要性（权重）。这类向量的经典权重计算方法是 TF-IDF。在信息检索领域，BM25 则是基于这种稀疏表示的成功且应用广泛的排序算法之一，其核心公式如下：
$$
\text{Score}(Q,D) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot 
\frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}
$$
其中：
- $IDF(q_i)$:将查询词$q_i$的逆文档频率，用于衡量一个词的普遍程度。越常见的词，IDF值越低。
- $f(q_i, D)$:查询词q_i在文档D的词频。
- $|D|$：文档D的长度
- avgdl:集合中所有文档的平均长度
- k_1,b:可调节的超参数。k_1用于控制词频饱和度（一个词在文档中出现10次和100次，其重要性增长并非线性）， b 用于控制文档长度归一化的程度。
这种方法虽然可解释性强，但是主要缺点是无法理解语义，例如它无法识别“汽车”和“轿车”是同义词，存在“词汇鸿沟”。

### 1.2 密集向量
密集向量，也常被称为“语义向量”，是通过深度学习模型学习到的数据（如文本、图像）的低维、稠密的浮点数表示。这些向量旨在将原始数据映射到一个连续的、充满意义的“语义空间”中来捕捉“语义”或“概念”。在理想的语义空间中，向量之间的距离和方向代表了它们所表示概念之间的关系。一个经典的例子是 `vector('国王') - vector('男人') + vector('女人')` 的计算结果在向量空间中非常接近 `vector('女王')`，这表明模型学会了“性别”和“皇室”这两个维度的抽象概念。它的代表包括 Word2Vec、GloVe、以及所有基于 Transformer 的模型（如 BERT、GPT）生成的嵌入（Embeddings）。
其主要优点是能够理解同义词、近义词和上下文关系，泛化能力强，在语义搜索任务中表现卓越。但缺点也同样明显：可解释性差（向量中的每个维度通常没有具体的物理意义），需要大量数据和算力进行模型训练，且对于未登录词（OOV）的处理相对困难。

## 二、混合检索
通过上文可以看出稀疏向量和密集向量各有千秋，那么将它们结合起来，实现优势互补，就成了一个不错的选择。混合检索便是基于这个思路，通过结合多种搜索算法（最常见的是稀疏与密集检索）来提升搜索结果相关性和召回率。

- **主要目标：**解决单一检索技术的局限性。例如，关键词检索无法理解语义，而向量检索则可能忽略掉必须精确匹配的关键词（如产品型号、函数名等）。混合检索旨在同时利用稀疏向量的**精确性**和密集向量的**泛化性**，以应对复杂多变的搜索需求。
### 2.1 技术原理与融合方法
混合检索通常并行执行两种检索算法，然后将两组异构的结果集融合成一个统一的排序列表。以下是两种主流的融合策略：
#### 2.1.1 倒数排序融合(Reciprocal Rank Fusion, RRF)
RRF 不关心不同检索系统的原始得分，只关心每个文档在各自结果集中的**排名**。其思想是：一个文档在不同检索系统中的排名越靠前，它的最终得分就越高。
其计分公式为
$$ RRF_{\text{score}}(d)=\sum_{i=1}^{k} \frac{1}{\operatorname{rank}_{i}(d)+c} $$
其中
- d是待评分的文档
- k是检索系统的数量
- rank_i(d)是文档d在第i个检索系统中排名
- c是一个常数（通常设为60），用于降低排名靠后文档的权重，避免对结果产生大影响。
#### 2.1.2 加权线性组合
这种方法需要先将不同检索系统的得分进行归一化（例如，统一到 0-1 区间），然后通过一个权重参数 `α` 来进行线性组合
$$Hybrid_{score} = \alpha \cdot Dense_{score} + (1  -\alpha)\cdot Sparse_{score}$$
通过调整 `α` 的值，可以灵活地控制语义相似性与关键词匹配在最终排序中的贡献比例。例如，在电商搜索中，可以调高关键词的权重；而在智能问答中，则可以侧重于语义。
### 2.2 优势与局限
| 优势                                       | 局限                                         |
| :----------------------------------------- | :------------------------------------------- |
| 召回率与准确率高：能同时捕获关键词和语义，显著优于单一检索。 | 计算资源消耗大：需要同时维护和查询两套索引。         |
| 灵活性强：可通过融合策略和权重调整，适应不同业务场景。   | 参数调试复杂：融合权重等超参数需要反复实验调优。       |
| 容错性好：关键词检索可部分弥补向量模型对拼写错误或罕见词的敏感性。 | 可解释性仍是挑战：融合后的结果排序理由难以直观分析。   |

## 三、代码实践：通过Milvus实现混合索引
接下来使用Milvus实现一个完整的混合检索流程，从定义 Schema、插入数据，到执行查询。
### 3.1 步骤一：定义Collection
**先创建一个Collection**
```python
import json
import os
# 设置HuggingFace镜像端点，解决国内访问问题
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import numpy as np
from pymilvus import connections, MilvusClient, FieldSchema, CollectionSchema, DataType, Collection, AnnSearchRequest, RRFRanker
from pymilvus.model.hybrid import BGEM3EmbeddingFunction  # 混合嵌入模型

# 1. 初始化设置
COLLECTION_NAME = "dragon_hybrid_demo"  # Milvus集合名称
MILVUS_URI = "http://localhost:19530"  # Milvus服务器地址和端口
DATA_PATH = "../../data/C4/metadata/dragon.json"  # 数据文件相对路径
BATCH_SIZE = 50  # 批处理大小，用于批量插入数据

# 2. 连接 Milvus 并初始化嵌入模型
print(f"--> 正在连接到 Milvus: {MILVUS_URI}")
connections.connect(uri=MILVUS_URI)  # 建立与Milvus数据库的连接

print("--> 正在初始化 BGE-M3 嵌入模型...")
# 初始化BGE-M3嵌入函数，使用FP32精度和CPU设备
ef = BGEM3EmbeddingFunction(use_fp16=False, device="cpu")
print(f"--> 嵌入模型初始化完成。密集向量维度: {ef.dim['dense']}")  # 输出密集向量维度

# 3. 创建 Collection
milvus_client = MilvusClient(uri=MILVUS_URI)  # 创建Milvus客户端实例
# 检查集合是否已存在，存在则删除
if milvus_client.has_collection(COLLECTION_NAME):
    print(f"--> 正在删除已存在的 Collection '{COLLECTION_NAME}'...")
    milvus_client.drop_collection(COLLECTION_NAME)  # 删除已存在的集合

# 定义集合的字段结构
fields = [
    FieldSchema(name="pk", dtype=DataType.VARCHAR, is_primary=True, auto_id=True, max_length=100),  # 主键，自动生成
    FieldSchema(name="img_id", dtype=DataType.VARCHAR, max_length=100),  # 图片ID
    FieldSchema(name="path", dtype=DataType.VARCHAR, max_length=256),  # 图片路径
    FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=256),  # 标题
    FieldSchema(name="description", dtype=DataType.VARCHAR, max_length=4096),  # 描述文本
    FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=64),  # 类别
    FieldSchema(name="location", dtype=DataType.VARCHAR, max_length=128),  # 位置信息
    FieldSchema(name="environment", dtype=DataType.VARCHAR, max_length=64),  # 环境信息
    FieldSchema(name="sparse_vector", dtype=DataType.SPARSE_FLOAT_VECTOR),  # 稀疏向量字段
    FieldSchema(name="dense_vector", dtype=DataType.FLOAT_VECTOR, dim=ef.dim["dense"])  # 密集向量字段，维度与模型匹配
]

# 如果集合不存在，则创建它及索引
if not milvus_client.has_collection(COLLECTION_NAME):
    print(f"--> 正在创建 Collection '{COLLECTION_NAME}'...")
    # 创建集合模式，包含字段定义和描述
    schema = CollectionSchema(fields, description="关于龙的混合检索示例")
    # 创建集合实例，使用强一致性级别
    collection = Collection(name=COLLECTION_NAME, schema=schema, consistency_level="Strong")
    print("--> Collection 创建成功。")

    # 创建索引
    print("--> 正在为新集合创建索引...")
    # 创建稀疏向量索引，使用SPARSE_INVERTED_INDEX类型和IP（内积）度量方式
    sparse_index = {"index_type": "SPARSE_INVERTED_INDEX", "metric_type": "IP"}
    collection.create_index("sparse_vector", sparse_index)
    print("稀疏向量索引创建成功。")

    # 创建密集向量索引，使用AUTOINDEX自动索引类型和IP度量方式
    dense_index = {"index_type": "AUTOINDEX", "metric_type": "IP"}
    collection.create_index("dense_vector", dense_index)
    print("密集向量索引创建成功。")

# 获取集合实例并加载到内存
collection = Collection(COLLECTION_NAME)
collection.load()  # 将集合数据加载到内存以提高查询性能
print(f"--> Collection '{COLLECTION_NAME}' 已加载到内存。")
```
**fields字段类型分析：**

- **pk**: 主键设计，`auto_id=True` 让 Milvus 自动生成唯一标识，避免主键冲突
- **标量字段**: 7个VARCHAR字段用于存储元数据，`max_length` 根据实际数据分布优化存储
- **稀疏向量**: `SPARSE_FLOAT_VECTOR` 类型，存储关键词权重
- **密集向量**: `FLOAT_VECTOR` 类型，固定1024维，存储语义特征
### 步骤二：BGE-M3双向量生成
使用 BGE-M3 作为向量生成器，它能够同时生成稀疏向量和密集向量。
#### 3.2.1 数据加载与预处理
```python
# 检查集合是否为空（没有数据）
if collection.is_empty:
    print(f"--> Collection 为空，开始插入数据...")
    
    # 读取JSON数据文件
    with open(DATA_PATH, 'r', encoding='utf-8') as f:
        dataset = json.load(f)  # 加载JSON数据到Python字典列表

    # 初始化两个列表用于存储处理后的数据
    docs, metadata = [], []
    
    # 遍历数据集中的每个项目
    for item in dataset:
        # 提取文本字段并组合成检索文档
        parts = [
            item.get('title', ''),        # 获取标题，若无则返回空字符串
            item.get('description', ''),  # 获取描述
            item.get('location', ''),     # 获取位置信息
            item.get('environment', ''), # 获取环境信息
        ]
        # 将非空部分用空格连接，形成完整的检索文本
        docs.append(' '.join(filter(None, parts)))
        
        # 保存原始元数据，用于后续插入到集合中
        metadata.append(item)
```
#### 3.2.2 向量生成
```python
print("--> 正在生成向量嵌入...")
embeddings = ef(docs)
print("--> 向量生成完成。")

# 获取两种向量
sparse_vectors = embeddings["sparse"]    # 稀疏向量：词频统计
dense_vectors = embeddings["dense"]      # 密集向量：语义编码
```
#### 3.2.3 Collection批量数据插入
```python
# 为每个字段准备批量数据
# 从metadata中提取各个字段的值，形成对应字段的数据列表
img_ids = [doc["img_id"] for doc in metadata]  # 提取所有图片ID
paths = [doc["path"] for doc in metadata]  # 提取所有图片路径
titles = [doc["title"] for doc in metadata]  # 提取所有标题
descriptions = [doc["description"] for doc in metadata]  # 提取所有描述文本
categories = [doc["category"] for doc in metadata]  # 提取所有分类信息
locations = [doc["location"] for doc in metadata]  # 提取所有位置信息
environments = [doc["environment"] for doc in metadata]  # 提取所有环境信息

# 插入数据到Milvus集合
# collection.insert() 方法参数说明：
# - 参数: 字段数据的列表，必须按照集合schema定义的字段顺序排列
# - 返回值: 包含插入结果的对象，通常包含插入的primary keys等信息
collection.insert([
    img_ids,           # 对应 img_id 字段
    paths,             # 对应 path 字段  
    titles,            # 对应 title 字段
    descriptions,      # 对应 description 字段
    categories,        # 对应 category 字段
    locations,         # 对应 location 字段
    environments,      # 对应 environment 字段
    sparse_vectors,    # 对应 sparse_vector 字段 (稀疏向量)
    dense_vectors      # 对应 dense_vector 字段 (密集向量)
])

# 确保数据持久化到磁盘
# collection.flush() 方法说明：
# - 功能: 将内存中的数据刷新到磁盘，确保数据持久化
# - 返回值: 刷新操作的结果状态
# - 注意: 在Milvus中，flush操作会确保所有插入的数据被写入存储
collection.flush()
```